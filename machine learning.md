# 机器学习笔记

## 正则化

$$
Loss + \lambda J(f)
$$
J(f) 是正则化项，一般与模型的复杂度成正比，模型越复杂，正则化值越大。

一般用参数向量的L1范数和L2范数。
$$
L1=\lambda || w||_{1}
$$

$$
L2=\frac{\lambda}{2}||w||^{2}
$$

L1就是向量中绝对值的和，L2就是平方和开方

##生成模型与判别模型

生成模型：由数据学习x与y的联合概率，然后再求出条件概率当作模型。
$$
P(Y|X)=\frac{P(X,Y)}{P(X)}
$$
比如HMM模型、朴素贝叶斯模型。好处就是收敛快？有隐变量也可以（无监督）。

判别模型：直接学习条件概率P(Y|X)，比如CRF模型、最大熵模型。

## 朴素贝叶斯模型

* 生成模型

  基于特征条件独立假设，学习输入输出的联合概率分布；然后在解码的时候，基于这个模型对给定输入x，利用贝叶斯定理求出后验概率最大的输出y。

* 怎么学习联合概率分布P(X,Y)?

  * $$
    P(X,Y)=P(Y)*P(X|Y)
    $$

  * 所以先学习P(Y),
    $$
    P(Y=c_{k})=\frac{\sum_{i=1}^N{I(y_{i}=c_{k})}}{N}
    $$

  * 然后再学习P(X|Y)
    $$
    P(X|Y)=P(x_{1},...,x_{n}|Y=c_{k}),k=1,2,...,K
    $$
    然而如果直接学习这个，就会有指数级别的参数数量。所以就需要条件独立假设。

  * 条件独立假设
    $$
    P(X=x|Y=c_{k})=P(x^{1},...,x^{n}|Y=c_{k})=\prod_{j=1}^nP(X^j=x^j|Y=c_{k})
    $$

  * 转而学习由条件独立拆开的
    $$
    P(x^j=a|Y=c_{k})=\frac{\sum_{i=1}^NI(x^j=a,y_{i}=c_{k})}{\sum_{i=1}^NI(y_{i}=c_{k})}
    $$

  * 最终

  $$
  y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
  $$

  * 然后因为分母一样，所以去掉分母：
    $$
    y=f(x)=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
    $$